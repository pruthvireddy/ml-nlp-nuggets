{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"logistic_regression_nlp.ipynb","provenance":[],"collapsed_sections":["AspwZmqdKEGg","LWJWQfoHKEGg"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"3d5smp5FKEGO"},"source":["# Assignment 1: Logistic Regression\n","Welcome to week one of this specialization. You will learn about logistic regression. Concretely, you will be implementing logistic regression for sentiment analysis on tweets. Given a tweet, you will decide if it has a positive sentiment or a negative one. Specifically you will: \n","\n","* Learn how to extract features for logistic regression given some text\n","* Implement logistic regression from scratch\n","* Apply logistic regression on a natural language processing task\n","* Test using your logistic regression\n","* Perform error analysis\n","\n","We will be using a data set of tweets. Hopefully you will get more than 99% accuracy.  \n","Run the cell below to load in the packages."],"id":"3d5smp5FKEGO"},{"cell_type":"markdown","metadata":{"id":"Tkzleb_-KEGT"},"source":["## Import functions and data"],"id":"Tkzleb_-KEGT"},{"cell_type":"code","metadata":{"id":"5abeH-37KEGU","outputId":"86d31859-5102-4064-e614-dac409041a5a"},"source":["# run this cell to import nltk\n","import nltk\n","from os import getcwd\n","import w1_unittest\n","\n","nltk.download('twitter_samples')\n","nltk.download('stopwords')"],"id":"5abeH-37KEGU","execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package twitter_samples to\n","[nltk_data]     /home/jovyan/nltk_data...\n","[nltk_data]   Unzipping corpora/twitter_samples.zip.\n","[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"wSddi3dHKEGX"},"source":["### Imported functions\n","\n","Download the data needed for this assignment. Check out the [documentation for the twitter_samples dataset](http://www.nltk.org/howto/twitter.html).\n","\n","* twitter_samples: if you're running this notebook on your local computer, you will need to download it using:\n","```Python\n","nltk.download('twitter_samples')\n","```\n","\n","* stopwords: if you're running this notebook on your local computer, you will need to download it using:\n","```python\n","nltk.download('stopwords')\n","```\n","\n","#### Import some helper functions that we provided in the utils.py file:\n","* process_tweet: cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\n","* build_freqs: this counts how often a word in the 'corpus' (the entire set of tweets) was associated with a positive label '1' or a negative label '0', then builds the 'freqs' dictionary, where each key is the (word,label) tuple, and the value is the count of its frequency within the corpus of tweets."],"id":"wSddi3dHKEGX"},{"cell_type":"code","metadata":{"id":"L2v2ZmRLKEGY"},"source":["filePath = f\"{getcwd()}/../tmp2/\"\n","nltk.data.path.append(filePath)"],"id":"L2v2ZmRLKEGY","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6D6smc5cKEGZ"},"source":["import numpy as np\n","import pandas as pd\n","from nltk.corpus import twitter_samples \n","\n","from utils import process_tweet, build_freqs"],"id":"6D6smc5cKEGZ","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FEvakfdAKEGZ"},"source":["### Prepare the data\n","* The `twitter_samples` contains subsets of five thousand positive_tweets, five thousand negative_tweets, and the full set of 10,000 tweets.  \n","    * If you used all three datasets, we would introduce duplicates of the positive tweets and negative tweets.  \n","    * You will select just the five thousand positive tweets and five thousand negative tweets."],"id":"FEvakfdAKEGZ"},{"cell_type":"code","metadata":{"id":"HPNgqZIIKEGa"},"source":["# select the set of positive and negative tweets\n","all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n","all_negative_tweets = twitter_samples.strings('negative_tweets.json')"],"id":"HPNgqZIIKEGa","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HOUQWS36KEGc"},"source":["* Train test split: 20% will be in the test set, and 80% in the training set.\n"],"id":"HOUQWS36KEGc"},{"cell_type":"code","metadata":{"id":"_ZrzRjtLKEGd"},"source":["# split the data into two pieces, one for training and one for testing (validation set) \n","test_pos = all_positive_tweets[4000:]\n","train_pos = all_positive_tweets[:4000]\n","test_neg = all_negative_tweets[4000:]\n","train_neg = all_negative_tweets[:4000]\n","\n","train_x = train_pos + train_neg \n","test_x = test_pos + test_neg"],"id":"_ZrzRjtLKEGd","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QLfmGlIkKEGe"},"source":["* Create the numpy array of positive labels and negative labels."],"id":"QLfmGlIkKEGe"},{"cell_type":"code","metadata":{"id":"89TuUU0GKEGe"},"source":["# combine positive and negative labels\n","train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n","test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"],"id":"89TuUU0GKEGe","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r7ojqDnxKEGe","outputId":"9bf98b87-713b-479d-dda5-c3f7d996b9de"},"source":["# Print the shape train and test sets\n","print(\"train_y.shape = \" + str(train_y.shape))\n","print(\"test_y.shape = \" + str(test_y.shape))"],"id":"r7ojqDnxKEGe","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["train_y.shape = (8000, 1)\n","test_y.shape = (2000, 1)\n"]}]},{"cell_type":"markdown","metadata":{"id":"uHT4vs3hKEGf"},"source":["* Create the frequency dictionary using the imported build_freqs function.  \n","    * We highly recommend that you open utils.py and read the build_freqs function to understand what it is doing.\n","    * To view the file directory, go to the menu and click File->Open.\n","\n","```Python\n","    for y,tweet in zip(ys, tweets):\n","        for word in process_tweet(tweet):\n","            pair = (word, y)\n","            if pair in freqs:\n","                freqs[pair] += 1\n","            else:\n","                freqs[pair] = 1\n","```\n","* Notice how the outer for loop goes through each tweet, and the inner for loop steps through each word in a tweet.\n","* The 'freqs' dictionary is the frequency dictionary that's being built. \n","* The key is the tuple (word, label), such as (\"happy\",1) or (\"happy\",0).  The value stored for each key is the count of how many times the word \"happy\" was associated with a positive label, or how many times \"happy\" was associated with a negative label."],"id":"uHT4vs3hKEGf"},{"cell_type":"code","metadata":{"id":"fheoISt3KEGf","outputId":"ef09c9cc-3906-4b5e-a440-8fb235ab79e8"},"source":["# create frequency dictionary\n","freqs = build_freqs(train_x, train_y)\n","\n","# check the output\n","print(\"type(freqs) = \" + str(type(freqs)))\n","print(\"len(freqs) = \" + str(len(freqs.keys())))"],"id":"fheoISt3KEGf","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["type(freqs) = <class 'dict'>\n","len(freqs) = 11436\n"]}]},{"cell_type":"markdown","metadata":{"id":"AspwZmqdKEGg"},"source":["#### Expected output\n","```\n","type(freqs) = <class 'dict'>\n","len(freqs) = 11436\n","```"],"id":"AspwZmqdKEGg"},{"cell_type":"markdown","metadata":{"id":"AB1KV42IKEGg"},"source":["### Process tweet\n","The given function 'process_tweet' tokenizes the tweet into individual words, removes stop words and applies stemming."],"id":"AB1KV42IKEGg"},{"cell_type":"code","metadata":{"id":"L6w8b_V3KEGg","outputId":"802ac5fb-bed0-45ed-bd89-df91fa839b4a"},"source":["# test the function below\n","print('This is an example of a positive tweet: \\n', train_x[0])\n","print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"],"id":"L6w8b_V3KEGg","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["This is an example of a positive tweet: \n"," #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n","\n","This is an example of the processed version of the tweet: \n"," ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"]}]},{"cell_type":"markdown","metadata":{"id":"LWJWQfoHKEGg"},"source":["#### Expected output\n","```\n","This is an example of a positive tweet: \n"," #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n"," \n","This is an example of the processes version: \n"," ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n","```"],"id":"LWJWQfoHKEGg"}]}