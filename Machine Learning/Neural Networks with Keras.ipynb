{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Neural Networks with Keras.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"qecTR8_jAH9Z"},"source":["# Install tensorflow and keras libraries first.  Code in command prompt:\n","##     conda install -c conda-forge tensorflow, keras\n","\n","import numpy as np\n","from tensorflow.keras.models import Sequential\n","\n","# The core data structure of Keras is a model, a way to organize layers.\n","\n","model = Sequential() # Define the architecture of you model using Sequential.  \n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZJO44sCCAH9f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623273507291,"user_tz":240,"elapsed":235,"user":{"displayName":"Michael D. Parrott","photoUrl":"","userId":"13070067452828357902"}},"outputId":"a4121b8c-989c-4d72-a4ad-c5a74ecdbb49"},"source":["#Build layers with Dense, followed by Activation()...\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation\n","\n","# two hidden layers with 32 nodes\n","# Activation is set to relu\n","# one output layer with 10 categories.  \n","# softmax function used to calculate 0 to 1 probabilities for each of 10 categories\n","\n","model = Sequential([\n","    Dense(32, input_shape=(784,)),\n","    Activation('relu'),\n","    Dense(32),\n","    Activation('relu'),\n","    Dense(1),\n","    Activation('softmax'),\n","])\n","\n","model.summary()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense (Dense)                (None, 32)                25120     \n","_________________________________________________________________\n","activation (Activation)      (None, 32)                0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 32)                1056      \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 32)                0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 33        \n","_________________________________________________________________\n","activation_2 (Activation)    (None, 1)                 0         \n","=================================================================\n","Total params: 26,209\n","Trainable params: 26,209\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JYWdU1yOAH9i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623273513223,"user_tz":240,"elapsed":161,"user":{"displayName":"Michael D. Parrott","photoUrl":"","userId":"13070067452828357902"}},"outputId":"b59ff05d-d914-45ec-f6c4-45c6164a13b3"},"source":["# model with two hidden layers\n","\n","model = Sequential([\n","    Dense(32, input_shape=(784,)),\n","    Activation('relu'),\n","    Dense(32),\n","    Activation('relu'),\n","    Dense(32),\n","    Activation('relu'),\n","    Dense(10),\n","    Activation('softmax'),\n","])\n","\n","\n","model.summary()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_3 (Dense)              (None, 32)                25120     \n","_________________________________________________________________\n","activation_3 (Activation)    (None, 32)                0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 32)                1056      \n","_________________________________________________________________\n","activation_4 (Activation)    (None, 32)                0         \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 32)                1056      \n","_________________________________________________________________\n","activation_5 (Activation)    (None, 32)                0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 10)                330       \n","_________________________________________________________________\n","activation_6 (Activation)    (None, 10)                0         \n","=================================================================\n","Total params: 27,562\n","Trainable params: 27,562\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LBixh7ZeAH9k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623273575739,"user_tz":240,"elapsed":184,"user":{"displayName":"Michael D. Parrott","photoUrl":"","userId":"13070067452828357902"}},"outputId":"c5c817d4-e66f-4df7-d22a-f05010f81fbb"},"source":["#Or build a model in steps using .add():\n","\n","from tensorflow.keras.layers import Dense\n","\n","model = Sequential() \n","model.add(Dense(units=64, activation='relu', input_dim=784))\n","model.add(Dense(units=64, activation='relu'))\n","model.add(Dense(units=10, activation='softmax'))\n","\n","model.summary()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_7 (Dense)              (None, 64)                50240     \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 64)                4160      \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 10)                650       \n","=================================================================\n","Total params: 55,050\n","Trainable params: 55,050\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SXucdUgJAH9n"},"source":["# Once your model looks good, configure its learning process with .compile():\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer='sgd',\n","              metrics=['accuracy'])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_S2EiUcAAH9p"},"source":["**loss can be set to:**\n","    - 'categorical_crossentropy' for multiple categories\n","    - 'binary_crossentropy' for binary categories\n","    - 'mse' for regression, which calculates the mse"]},{"cell_type":"markdown","metadata":{"id":"4YYoImfwAH9p"},"source":["**optimizer can be set to 'sgd' for stochastic gradient descent or a variety of other techniques.** "]},{"cell_type":"markdown","metadata":{"id":"HSGiGFwfAH9q"},"source":["## Training a keras model\n","\n","Keras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the  fit function."]},{"cell_type":"code","metadata":{"id":"M4zJrGrjAH9q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623274091002,"user_tz":240,"elapsed":10974,"user":{"displayName":"Michael D. Parrott","photoUrl":"","userId":"13070067452828357902"}},"outputId":"98652794-7023-4fd4-a676-7e8730b7b8ce"},"source":["# For a single-input model with 2 classes (binary classification):\n","\n","model = Sequential()\n","model.add(Dense(32, activation='relu', input_dim=100))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(optimizer='sgd',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy']) # Change to 'AUC' for ROC area under the curve\n","\n","# Generate dummy dataq7_model\n","import numpy as np\n","data = np.random.random((1000, 100)) # X data\n","labels = np.random.randint(2, size=(1000, 1)) # y data\n","\n","# Train the model, iterating on the data in batches of 32 samples\n","model.fit(data, labels, validation_split=0.20, epochs=100, batch_size=32)\n","\n","#Note that you can also use train_test_split() with , validation_data=(X_test,y_test) argument from Keras in same manner.\n","##Split data first and then simply train on training data and add test data to this argument.\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","25/25 [==============================] - 1s 9ms/step - loss: 0.7094 - accuracy: 0.4888 - val_loss: 0.7058 - val_accuracy: 0.4700\n","Epoch 2/100\n","25/25 [==============================] - 0s 3ms/step - loss: 0.7044 - accuracy: 0.4950 - val_loss: 0.7058 - val_accuracy: 0.4700\n","Epoch 3/100\n","25/25 [==============================] - 0s 3ms/step - loss: 0.7032 - accuracy: 0.5025 - val_loss: 0.7060 - val_accuracy: 0.4600\n","Epoch 4/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.7014 - accuracy: 0.5013 - val_loss: 0.7066 - val_accuracy: 0.4600\n","Epoch 5/100\n","25/25 [==============================] - 0s 3ms/step - loss: 0.7005 - accuracy: 0.5150 - val_loss: 0.7064 - val_accuracy: 0.4500\n","Epoch 6/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6992 - accuracy: 0.5125 - val_loss: 0.7071 - val_accuracy: 0.4600\n","Epoch 7/100\n","25/25 [==============================] - 0s 3ms/step - loss: 0.6984 - accuracy: 0.5200 - val_loss: 0.7066 - val_accuracy: 0.4600\n","Epoch 8/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6980 - accuracy: 0.5138 - val_loss: 0.7052 - val_accuracy: 0.4600\n","Epoch 9/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6966 - accuracy: 0.5263 - val_loss: 0.7048 - val_accuracy: 0.4800\n","Epoch 10/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6969 - accuracy: 0.5163 - val_loss: 0.7045 - val_accuracy: 0.4750\n","Epoch 11/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6950 - accuracy: 0.5113 - val_loss: 0.7043 - val_accuracy: 0.4750\n","Epoch 12/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6942 - accuracy: 0.5163 - val_loss: 0.7041 - val_accuracy: 0.4750\n","Epoch 13/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.5200 - val_loss: 0.7040 - val_accuracy: 0.4650\n","Epoch 14/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5412 - val_loss: 0.7033 - val_accuracy: 0.4900\n","Epoch 15/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5200 - val_loss: 0.7032 - val_accuracy: 0.4850\n","Epoch 16/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5275 - val_loss: 0.7034 - val_accuracy: 0.4800\n","Epoch 17/100\n","25/25 [==============================] - 0s 3ms/step - loss: 0.6909 - accuracy: 0.5337 - val_loss: 0.7028 - val_accuracy: 0.4850\n","Epoch 18/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6908 - accuracy: 0.5312 - val_loss: 0.7040 - val_accuracy: 0.4550\n","Epoch 19/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6901 - accuracy: 0.5300 - val_loss: 0.7039 - val_accuracy: 0.4500\n","Epoch 20/100\n","25/25 [==============================] - 0s 3ms/step - loss: 0.6897 - accuracy: 0.5412 - val_loss: 0.7037 - val_accuracy: 0.4550\n","Epoch 21/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6885 - accuracy: 0.5350 - val_loss: 0.7037 - val_accuracy: 0.4550\n","Epoch 22/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6883 - accuracy: 0.5400 - val_loss: 0.7026 - val_accuracy: 0.4900\n","Epoch 23/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6877 - accuracy: 0.5375 - val_loss: 0.7040 - val_accuracy: 0.4650\n","Epoch 24/100\n","25/25 [==============================] - 0s 3ms/step - loss: 0.6870 - accuracy: 0.5350 - val_loss: 0.7031 - val_accuracy: 0.4650\n","Epoch 25/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6870 - accuracy: 0.5312 - val_loss: 0.7027 - val_accuracy: 0.4800\n","Epoch 26/100\n","25/25 [==============================] - 0s 3ms/step - loss: 0.6861 - accuracy: 0.5425 - val_loss: 0.7044 - val_accuracy: 0.4850\n","Epoch 27/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6857 - accuracy: 0.5400 - val_loss: 0.7048 - val_accuracy: 0.4800\n","Epoch 28/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6857 - accuracy: 0.5362 - val_loss: 0.7036 - val_accuracy: 0.4650\n","Epoch 29/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6843 - accuracy: 0.5475 - val_loss: 0.7039 - val_accuracy: 0.4700\n","Epoch 30/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6839 - accuracy: 0.5475 - val_loss: 0.7035 - val_accuracy: 0.4800\n","Epoch 31/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6831 - accuracy: 0.5487 - val_loss: 0.7032 - val_accuracy: 0.4750\n","Epoch 32/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6828 - accuracy: 0.5500 - val_loss: 0.7039 - val_accuracy: 0.4750\n","Epoch 33/100\n","25/25 [==============================] - 0s 3ms/step - loss: 0.6826 - accuracy: 0.5600 - val_loss: 0.7040 - val_accuracy: 0.4650\n","Epoch 34/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6819 - accuracy: 0.5550 - val_loss: 0.7036 - val_accuracy: 0.4800\n","Epoch 35/100\n","25/25 [==============================] - 0s 3ms/step - loss: 0.6814 - accuracy: 0.5625 - val_loss: 0.7038 - val_accuracy: 0.4750\n","Epoch 36/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6811 - accuracy: 0.5525 - val_loss: 0.7030 - val_accuracy: 0.4950\n","Epoch 37/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6803 - accuracy: 0.5638 - val_loss: 0.7035 - val_accuracy: 0.4850\n","Epoch 38/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6798 - accuracy: 0.5587 - val_loss: 0.7043 - val_accuracy: 0.4750\n","Epoch 39/100\n","25/25 [==============================] - 0s 3ms/step - loss: 0.6794 - accuracy: 0.5625 - val_loss: 0.7031 - val_accuracy: 0.4900\n","Epoch 40/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6787 - accuracy: 0.5587 - val_loss: 0.7029 - val_accuracy: 0.5000\n","Epoch 41/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6788 - accuracy: 0.5713 - val_loss: 0.7039 - val_accuracy: 0.4750\n","Epoch 42/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6780 - accuracy: 0.5700 - val_loss: 0.7027 - val_accuracy: 0.5000\n","Epoch 43/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6776 - accuracy: 0.5725 - val_loss: 0.7033 - val_accuracy: 0.5000\n","Epoch 44/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6775 - accuracy: 0.5663 - val_loss: 0.7029 - val_accuracy: 0.5000\n","Epoch 45/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6765 - accuracy: 0.5688 - val_loss: 0.7028 - val_accuracy: 0.5000\n","Epoch 46/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6764 - accuracy: 0.5738 - val_loss: 0.7028 - val_accuracy: 0.5000\n","Epoch 47/100\n","25/25 [==============================] - 0s 3ms/step - loss: 0.6764 - accuracy: 0.5738 - val_loss: 0.7026 - val_accuracy: 0.5100\n","Epoch 48/100\n","25/25 [==============================] - 0s 3ms/step - loss: 0.6750 - accuracy: 0.5825 - val_loss: 0.7037 - val_accuracy: 0.4800\n","Epoch 49/100\n","25/25 [==============================] - 0s 3ms/step - loss: 0.6751 - accuracy: 0.5763 - val_loss: 0.7031 - val_accuracy: 0.5050\n","Epoch 50/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6748 - accuracy: 0.5700 - val_loss: 0.7027 - val_accuracy: 0.5100\n","Epoch 51/100\n","25/25 [==============================] - 0s 3ms/step - loss: 0.6743 - accuracy: 0.5825 - val_loss: 0.7037 - val_accuracy: 0.4950\n","Epoch 52/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6736 - accuracy: 0.5875 - val_loss: 0.7030 - val_accuracy: 0.5100\n","Epoch 53/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6740 - accuracy: 0.5775 - val_loss: 0.7021 - val_accuracy: 0.5150\n","Epoch 54/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6729 - accuracy: 0.6025 - val_loss: 0.7026 - val_accuracy: 0.5150\n","Epoch 55/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6724 - accuracy: 0.5788 - val_loss: 0.7023 - val_accuracy: 0.5250\n","Epoch 56/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6721 - accuracy: 0.5950 - val_loss: 0.7033 - val_accuracy: 0.5150\n","Epoch 57/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6716 - accuracy: 0.5987 - val_loss: 0.7028 - val_accuracy: 0.5150\n","Epoch 58/100\n","25/25 [==============================] - 0s 3ms/step - loss: 0.6712 - accuracy: 0.6075 - val_loss: 0.7034 - val_accuracy: 0.5150\n","Epoch 59/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6701 - accuracy: 0.5938 - val_loss: 0.7033 - val_accuracy: 0.5200\n","Epoch 60/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6704 - accuracy: 0.6025 - val_loss: 0.7034 - val_accuracy: 0.5200\n","Epoch 61/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6698 - accuracy: 0.6012 - val_loss: 0.7040 - val_accuracy: 0.5050\n","Epoch 62/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6694 - accuracy: 0.6000 - val_loss: 0.7033 - val_accuracy: 0.5250\n","Epoch 63/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6693 - accuracy: 0.6087 - val_loss: 0.7040 - val_accuracy: 0.5150\n","Epoch 64/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6687 - accuracy: 0.6087 - val_loss: 0.7047 - val_accuracy: 0.4700\n","Epoch 65/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6679 - accuracy: 0.5975 - val_loss: 0.7033 - val_accuracy: 0.5250\n","Epoch 66/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6679 - accuracy: 0.6075 - val_loss: 0.7038 - val_accuracy: 0.5350\n","Epoch 67/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6675 - accuracy: 0.6112 - val_loss: 0.7035 - val_accuracy: 0.5350\n","Epoch 68/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6668 - accuracy: 0.6025 - val_loss: 0.7037 - val_accuracy: 0.5300\n","Epoch 69/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6666 - accuracy: 0.6012 - val_loss: 0.7035 - val_accuracy: 0.5250\n","Epoch 70/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6659 - accuracy: 0.6112 - val_loss: 0.7042 - val_accuracy: 0.5250\n","Epoch 71/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6656 - accuracy: 0.6100 - val_loss: 0.7046 - val_accuracy: 0.5250\n","Epoch 72/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6654 - accuracy: 0.6100 - val_loss: 0.7039 - val_accuracy: 0.5350\n","Epoch 73/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6648 - accuracy: 0.6100 - val_loss: 0.7036 - val_accuracy: 0.5250\n","Epoch 74/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6647 - accuracy: 0.6075 - val_loss: 0.7044 - val_accuracy: 0.5250\n","Epoch 75/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6646 - accuracy: 0.6075 - val_loss: 0.7041 - val_accuracy: 0.5350\n","Epoch 76/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6632 - accuracy: 0.6112 - val_loss: 0.7058 - val_accuracy: 0.4850\n","Epoch 77/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6631 - accuracy: 0.5987 - val_loss: 0.7044 - val_accuracy: 0.5350\n","Epoch 78/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6633 - accuracy: 0.6087 - val_loss: 0.7048 - val_accuracy: 0.5250\n","Epoch 79/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6619 - accuracy: 0.6200 - val_loss: 0.7055 - val_accuracy: 0.5000\n","Epoch 80/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6617 - accuracy: 0.6137 - val_loss: 0.7047 - val_accuracy: 0.5300\n","Epoch 81/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6612 - accuracy: 0.6175 - val_loss: 0.7054 - val_accuracy: 0.5250\n","Epoch 82/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6608 - accuracy: 0.6187 - val_loss: 0.7052 - val_accuracy: 0.5300\n","Epoch 83/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6602 - accuracy: 0.6187 - val_loss: 0.7058 - val_accuracy: 0.5050\n","Epoch 84/100\n","25/25 [==============================] - 0s 3ms/step - loss: 0.6599 - accuracy: 0.6237 - val_loss: 0.7063 - val_accuracy: 0.4950\n","Epoch 85/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6597 - accuracy: 0.6200 - val_loss: 0.7063 - val_accuracy: 0.5000\n","Epoch 86/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6589 - accuracy: 0.6250 - val_loss: 0.7055 - val_accuracy: 0.5250\n","Epoch 87/100\n","25/25 [==============================] - 0s 3ms/step - loss: 0.6582 - accuracy: 0.6263 - val_loss: 0.7077 - val_accuracy: 0.4800\n","Epoch 88/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6577 - accuracy: 0.6200 - val_loss: 0.7055 - val_accuracy: 0.5200\n","Epoch 89/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6575 - accuracy: 0.6275 - val_loss: 0.7058 - val_accuracy: 0.5300\n","Epoch 90/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6573 - accuracy: 0.6313 - val_loss: 0.7062 - val_accuracy: 0.5400\n","Epoch 91/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6568 - accuracy: 0.6225 - val_loss: 0.7061 - val_accuracy: 0.5200\n","Epoch 92/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6558 - accuracy: 0.6275 - val_loss: 0.7077 - val_accuracy: 0.4850\n","Epoch 93/100\n","25/25 [==============================] - 0s 3ms/step - loss: 0.6563 - accuracy: 0.6263 - val_loss: 0.7077 - val_accuracy: 0.4950\n","Epoch 94/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6551 - accuracy: 0.6313 - val_loss: 0.7061 - val_accuracy: 0.5200\n","Epoch 95/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6551 - accuracy: 0.6313 - val_loss: 0.7073 - val_accuracy: 0.5200\n","Epoch 96/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6547 - accuracy: 0.6263 - val_loss: 0.7063 - val_accuracy: 0.5250\n","Epoch 97/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6546 - accuracy: 0.6475 - val_loss: 0.7065 - val_accuracy: 0.5250\n","Epoch 98/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6538 - accuracy: 0.6313 - val_loss: 0.7075 - val_accuracy: 0.5150\n","Epoch 99/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6535 - accuracy: 0.6313 - val_loss: 0.7076 - val_accuracy: 0.5100\n","Epoch 100/100\n","25/25 [==============================] - 0s 2ms/step - loss: 0.6526 - accuracy: 0.6375 - val_loss: 0.7086 - val_accuracy: 0.5000\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f51e289cf10>"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OZ63Y3-psDQt","executionInfo":{"status":"ok","timestamp":1623274307267,"user_tz":240,"elapsed":232,"user":{"displayName":"Michael D. Parrott","photoUrl":"","userId":"13070067452828357902"}},"outputId":"f93f7dde-37fa-457e-b2fd-0bc23606c4eb"},"source":["np.random.randint(10, size=(1000, 1))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0],\n","       [4],\n","       [0],\n","       [0],\n","       [5],\n","       [5],\n","       [5],\n","       [3],\n","       [3],\n","       [9],\n","       [2],\n","       [7],\n","       [6],\n","       [5],\n","       [6],\n","       [2],\n","       [6],\n","       [0],\n","       [9],\n","       [5],\n","       [8],\n","       [4],\n","       [0],\n","       [0],\n","       [7],\n","       [5],\n","       [6],\n","       [3],\n","       [9],\n","       [2],\n","       [5],\n","       [1],\n","       [5],\n","       [6],\n","       [8],\n","       [1],\n","       [3],\n","       [7],\n","       [4],\n","       [3],\n","       [5],\n","       [0],\n","       [4],\n","       [1],\n","       [3],\n","       [8],\n","       [9],\n","       [5],\n","       [2],\n","       [5],\n","       [6],\n","       [6],\n","       [6],\n","       [8],\n","       [5],\n","       [4],\n","       [2],\n","       [4],\n","       [0],\n","       [3],\n","       [9],\n","       [3],\n","       [1],\n","       [5],\n","       [0],\n","       [1],\n","       [5],\n","       [4],\n","       [1],\n","       [2],\n","       [6],\n","       [1],\n","       [7],\n","       [7],\n","       [2],\n","       [7],\n","       [0],\n","       [7],\n","       [8],\n","       [8],\n","       [6],\n","       [4],\n","       [6],\n","       [0],\n","       [4],\n","       [7],\n","       [6],\n","       [0],\n","       [9],\n","       [6],\n","       [3],\n","       [2],\n","       [9],\n","       [8],\n","       [2],\n","       [8],\n","       [6],\n","       [1],\n","       [2],\n","       [8],\n","       [6],\n","       [9],\n","       [3],\n","       [8],\n","       [5],\n","       [5],\n","       [5],\n","       [9],\n","       [2],\n","       [7],\n","       [1],\n","       [8],\n","       [1],\n","       [0],\n","       [0],\n","       [0],\n","       [2],\n","       [9],\n","       [5],\n","       [5],\n","       [4],\n","       [4],\n","       [9],\n","       [7],\n","       [6],\n","       [8],\n","       [3],\n","       [7],\n","       [0],\n","       [6],\n","       [7],\n","       [2],\n","       [7],\n","       [6],\n","       [7],\n","       [0],\n","       [0],\n","       [8],\n","       [0],\n","       [6],\n","       [3],\n","       [1],\n","       [6],\n","       [1],\n","       [4],\n","       [7],\n","       [4],\n","       [3],\n","       [9],\n","       [6],\n","       [7],\n","       [6],\n","       [6],\n","       [2],\n","       [6],\n","       [0],\n","       [9],\n","       [3],\n","       [4],\n","       [6],\n","       [7],\n","       [6],\n","       [0],\n","       [4],\n","       [0],\n","       [9],\n","       [9],\n","       [0],\n","       [3],\n","       [9],\n","       [8],\n","       [0],\n","       [2],\n","       [1],\n","       [4],\n","       [6],\n","       [6],\n","       [5],\n","       [5],\n","       [6],\n","       [7],\n","       [8],\n","       [3],\n","       [8],\n","       [6],\n","       [5],\n","       [8],\n","       [1],\n","       [5],\n","       [4],\n","       [1],\n","       [6],\n","       [6],\n","       [4],\n","       [3],\n","       [3],\n","       [4],\n","       [4],\n","       [9],\n","       [0],\n","       [6],\n","       [1],\n","       [5],\n","       [7],\n","       [8],\n","       [1],\n","       [9],\n","       [7],\n","       [0],\n","       [8],\n","       [3],\n","       [5],\n","       [2],\n","       [5],\n","       [5],\n","       [1],\n","       [2],\n","       [2],\n","       [9],\n","       [4],\n","       [8],\n","       [2],\n","       [3],\n","       [0],\n","       [4],\n","       [7],\n","       [6],\n","       [1],\n","       [0],\n","       [2],\n","       [3],\n","       [3],\n","       [8],\n","       [0],\n","       [2],\n","       [3],\n","       [1],\n","       [3],\n","       [4],\n","       [4],\n","       [5],\n","       [0],\n","       [0],\n","       [4],\n","       [8],\n","       [8],\n","       [4],\n","       [6],\n","       [9],\n","       [5],\n","       [0],\n","       [4],\n","       [5],\n","       [8],\n","       [8],\n","       [0],\n","       [2],\n","       [2],\n","       [4],\n","       [9],\n","       [9],\n","       [2],\n","       [4],\n","       [9],\n","       [2],\n","       [2],\n","       [0],\n","       [9],\n","       [9],\n","       [4],\n","       [9],\n","       [8],\n","       [5],\n","       [3],\n","       [2],\n","       [0],\n","       [6],\n","       [4],\n","       [3],\n","       [9],\n","       [0],\n","       [4],\n","       [9],\n","       [7],\n","       [8],\n","       [7],\n","       [7],\n","       [6],\n","       [0],\n","       [4],\n","       [3],\n","       [5],\n","       [2],\n","       [9],\n","       [3],\n","       [4],\n","       [1],\n","       [2],\n","       [1],\n","       [4],\n","       [6],\n","       [7],\n","       [6],\n","       [2],\n","       [6],\n","       [8],\n","       [5],\n","       [1],\n","       [4],\n","       [3],\n","       [8],\n","       [7],\n","       [4],\n","       [0],\n","       [7],\n","       [2],\n","       [8],\n","       [7],\n","       [5],\n","       [5],\n","       [1],\n","       [4],\n","       [3],\n","       [3],\n","       [6],\n","       [9],\n","       [1],\n","       [7],\n","       [8],\n","       [1],\n","       [1],\n","       [2],\n","       [7],\n","       [4],\n","       [5],\n","       [3],\n","       [9],\n","       [4],\n","       [5],\n","       [1],\n","       [9],\n","       [2],\n","       [3],\n","       [7],\n","       [5],\n","       [2],\n","       [4],\n","       [1],\n","       [6],\n","       [2],\n","       [2],\n","       [7],\n","       [9],\n","       [1],\n","       [2],\n","       [0],\n","       [0],\n","       [3],\n","       [7],\n","       [8],\n","       [6],\n","       [5],\n","       [5],\n","       [4],\n","       [9],\n","       [4],\n","       [6],\n","       [2],\n","       [6],\n","       [5],\n","       [0],\n","       [8],\n","       [7],\n","       [0],\n","       [4],\n","       [5],\n","       [1],\n","       [4],\n","       [9],\n","       [9],\n","       [1],\n","       [5],\n","       [4],\n","       [0],\n","       [1],\n","       [2],\n","       [6],\n","       [9],\n","       [8],\n","       [5],\n","       [8],\n","       [0],\n","       [5],\n","       [4],\n","       [2],\n","       [1],\n","       [8],\n","       [3],\n","       [4],\n","       [5],\n","       [9],\n","       [6],\n","       [0],\n","       [5],\n","       [2],\n","       [5],\n","       [8],\n","       [4],\n","       [7],\n","       [1],\n","       [8],\n","       [4],\n","       [5],\n","       [0],\n","       [8],\n","       [6],\n","       [0],\n","       [8],\n","       [6],\n","       [4],\n","       [6],\n","       [3],\n","       [0],\n","       [5],\n","       [6],\n","       [0],\n","       [4],\n","       [8],\n","       [4],\n","       [8],\n","       [3],\n","       [2],\n","       [2],\n","       [9],\n","       [3],\n","       [5],\n","       [2],\n","       [1],\n","       [8],\n","       [9],\n","       [1],\n","       [4],\n","       [6],\n","       [9],\n","       [8],\n","       [6],\n","       [2],\n","       [9],\n","       [7],\n","       [7],\n","       [2],\n","       [3],\n","       [3],\n","       [2],\n","       [1],\n","       [4],\n","       [3],\n","       [4],\n","       [8],\n","       [0],\n","       [9],\n","       [0],\n","       [7],\n","       [9],\n","       [8],\n","       [1],\n","       [6],\n","       [2],\n","       [4],\n","       [0],\n","       [5],\n","       [7],\n","       [9],\n","       [5],\n","       [4],\n","       [0],\n","       [9],\n","       [6],\n","       [6],\n","       [7],\n","       [8],\n","       [2],\n","       [5],\n","       [3],\n","       [5],\n","       [3],\n","       [7],\n","       [7],\n","       [6],\n","       [6],\n","       [0],\n","       [6],\n","       [4],\n","       [1],\n","       [7],\n","       [5],\n","       [5],\n","       [9],\n","       [3],\n","       [5],\n","       [2],\n","       [8],\n","       [1],\n","       [4],\n","       [3],\n","       [0],\n","       [8],\n","       [2],\n","       [8],\n","       [6],\n","       [1],\n","       [2],\n","       [6],\n","       [6],\n","       [1],\n","       [4],\n","       [0],\n","       [0],\n","       [0],\n","       [4],\n","       [1],\n","       [0],\n","       [5],\n","       [4],\n","       [3],\n","       [3],\n","       [1],\n","       [7],\n","       [2],\n","       [8],\n","       [9],\n","       [4],\n","       [6],\n","       [1],\n","       [2],\n","       [3],\n","       [2],\n","       [8],\n","       [1],\n","       [4],\n","       [5],\n","       [1],\n","       [2],\n","       [5],\n","       [8],\n","       [6],\n","       [2],\n","       [7],\n","       [2],\n","       [7],\n","       [1],\n","       [6],\n","       [7],\n","       [0],\n","       [9],\n","       [2],\n","       [8],\n","       [3],\n","       [1],\n","       [0],\n","       [4],\n","       [7],\n","       [4],\n","       [9],\n","       [6],\n","       [6],\n","       [2],\n","       [2],\n","       [9],\n","       [7],\n","       [5],\n","       [3],\n","       [2],\n","       [1],\n","       [6],\n","       [0],\n","       [3],\n","       [7],\n","       [1],\n","       [5],\n","       [8],\n","       [0],\n","       [2],\n","       [9],\n","       [5],\n","       [9],\n","       [9],\n","       [6],\n","       [7],\n","       [8],\n","       [7],\n","       [5],\n","       [5],\n","       [7],\n","       [1],\n","       [9],\n","       [1],\n","       [3],\n","       [4],\n","       [6],\n","       [4],\n","       [8],\n","       [7],\n","       [2],\n","       [7],\n","       [5],\n","       [1],\n","       [5],\n","       [9],\n","       [6],\n","       [0],\n","       [4],\n","       [6],\n","       [5],\n","       [2],\n","       [8],\n","       [7],\n","       [3],\n","       [7],\n","       [0],\n","       [7],\n","       [8],\n","       [2],\n","       [9],\n","       [1],\n","       [4],\n","       [2],\n","       [8],\n","       [6],\n","       [0],\n","       [2],\n","       [9],\n","       [3],\n","       [7],\n","       [9],\n","       [3],\n","       [6],\n","       [3],\n","       [3],\n","       [3],\n","       [7],\n","       [1],\n","       [4],\n","       [4],\n","       [8],\n","       [6],\n","       [8],\n","       [6],\n","       [4],\n","       [3],\n","       [3],\n","       [1],\n","       [5],\n","       [5],\n","       [8],\n","       [5],\n","       [9],\n","       [3],\n","       [4],\n","       [6],\n","       [2],\n","       [1],\n","       [8],\n","       [1],\n","       [7],\n","       [9],\n","       [0],\n","       [5],\n","       [7],\n","       [1],\n","       [2],\n","       [3],\n","       [6],\n","       [2],\n","       [4],\n","       [2],\n","       [8],\n","       [6],\n","       [2],\n","       [8],\n","       [6],\n","       [7],\n","       [0],\n","       [6],\n","       [8],\n","       [0],\n","       [3],\n","       [6],\n","       [9],\n","       [7],\n","       [1],\n","       [7],\n","       [7],\n","       [1],\n","       [1],\n","       [3],\n","       [3],\n","       [8],\n","       [1],\n","       [0],\n","       [3],\n","       [9],\n","       [2],\n","       [3],\n","       [1],\n","       [0],\n","       [7],\n","       [4],\n","       [1],\n","       [8],\n","       [8],\n","       [4],\n","       [2],\n","       [0],\n","       [8],\n","       [8],\n","       [7],\n","       [0],\n","       [8],\n","       [8],\n","       [7],\n","       [3],\n","       [7],\n","       [4],\n","       [5],\n","       [7],\n","       [7],\n","       [9],\n","       [3],\n","       [5],\n","       [5],\n","       [1],\n","       [4],\n","       [7],\n","       [9],\n","       [8],\n","       [0],\n","       [9],\n","       [6],\n","       [5],\n","       [3],\n","       [5],\n","       [6],\n","       [4],\n","       [5],\n","       [9],\n","       [6],\n","       [4],\n","       [0],\n","       [5],\n","       [2],\n","       [9],\n","       [2],\n","       [3],\n","       [4],\n","       [5],\n","       [9],\n","       [2],\n","       [3],\n","       [1],\n","       [8],\n","       [4],\n","       [7],\n","       [5],\n","       [8],\n","       [2],\n","       [5],\n","       [1],\n","       [3],\n","       [7],\n","       [1],\n","       [7],\n","       [7],\n","       [4],\n","       [7],\n","       [6],\n","       [2],\n","       [0],\n","       [8],\n","       [9],\n","       [5],\n","       [8],\n","       [1],\n","       [0],\n","       [0],\n","       [9],\n","       [8],\n","       [0],\n","       [2],\n","       [2],\n","       [3],\n","       [1],\n","       [9],\n","       [6],\n","       [7],\n","       [4],\n","       [6],\n","       [3],\n","       [6],\n","       [7],\n","       [3],\n","       [8],\n","       [4],\n","       [0],\n","       [1],\n","       [5],\n","       [5],\n","       [0],\n","       [5],\n","       [8],\n","       [8],\n","       [7],\n","       [0],\n","       [2],\n","       [5],\n","       [9],\n","       [0],\n","       [9],\n","       [9],\n","       [9],\n","       [1],\n","       [6],\n","       [7],\n","       [5],\n","       [8],\n","       [8],\n","       [3],\n","       [6],\n","       [6],\n","       [6],\n","       [7],\n","       [9],\n","       [0],\n","       [2],\n","       [3],\n","       [1],\n","       [3],\n","       [5],\n","       [1],\n","       [9],\n","       [4],\n","       [7],\n","       [8],\n","       [4],\n","       [1],\n","       [4],\n","       [6],\n","       [4],\n","       [7],\n","       [1],\n","       [5],\n","       [3],\n","       [0],\n","       [0],\n","       [3],\n","       [8],\n","       [6],\n","       [4],\n","       [3],\n","       [2],\n","       [9],\n","       [0],\n","       [4],\n","       [6],\n","       [3],\n","       [7],\n","       [8],\n","       [5],\n","       [8],\n","       [0],\n","       [5],\n","       [0],\n","       [9],\n","       [5],\n","       [9],\n","       [1],\n","       [9],\n","       [7],\n","       [9],\n","       [0],\n","       [1],\n","       [5],\n","       [6],\n","       [9],\n","       [9],\n","       [3],\n","       [5],\n","       [6],\n","       [3],\n","       [6],\n","       [3],\n","       [1],\n","       [1],\n","       [7],\n","       [6],\n","       [3],\n","       [4],\n","       [9],\n","       [2],\n","       [1],\n","       [9],\n","       [4],\n","       [9],\n","       [8],\n","       [2],\n","       [7],\n","       [0],\n","       [8],\n","       [1],\n","       [7],\n","       [1],\n","       [1],\n","       [2],\n","       [8],\n","       [8],\n","       [4],\n","       [9],\n","       [3],\n","       [4],\n","       [4],\n","       [3],\n","       [5],\n","       [1],\n","       [6],\n","       [2],\n","       [1],\n","       [7],\n","       [0],\n","       [9],\n","       [8],\n","       [4],\n","       [8],\n","       [1],\n","       [8],\n","       [7],\n","       [7],\n","       [0],\n","       [3],\n","       [3],\n","       [0],\n","       [3],\n","       [2],\n","       [4],\n","       [3],\n","       [4],\n","       [4],\n","       [9],\n","       [4],\n","       [0],\n","       [2],\n","       [9],\n","       [4],\n","       [6],\n","       [3],\n","       [3],\n","       [2],\n","       [7],\n","       [4],\n","       [2],\n","       [1],\n","       [4],\n","       [5],\n","       [7],\n","       [3],\n","       [4],\n","       [4],\n","       [1],\n","       [9],\n","       [2],\n","       [2],\n","       [0],\n","       [9],\n","       [7],\n","       [2],\n","       [8],\n","       [0],\n","       [7],\n","       [9],\n","       [9],\n","       [3],\n","       [5],\n","       [7],\n","       [6],\n","       [7],\n","       [1],\n","       [0],\n","       [5],\n","       [3],\n","       [6],\n","       [8],\n","       [4],\n","       [8],\n","       [4],\n","       [6],\n","       [9],\n","       [3],\n","       [6],\n","       [0],\n","       [4],\n","       [4]])"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"A9rrVgbrAH9t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623274508707,"user_tz":240,"elapsed":1345,"user":{"displayName":"Michael D. Parrott","photoUrl":"","userId":"13070067452828357902"}},"outputId":"62095fa5-6690-42d7-851e-5fb51ca2fdf2"},"source":["# for multiple categories you  need to one hot encode y using to_categorical()\n","\n","import tensorflow.keras as keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation\n","from tensorflow.keras.optimizers import SGD\n","\n","# Generate dummy data\n","import numpy as np\n","x_train = np.random.random((1000, 20))\n","y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n","x_test = np.random.random((100, 20))\n","y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n","\n","model = Sequential()\n","# Dense(64) is a fully-connected layer with 64 hidden units.\n","# in the first layer, you must specify the expected input data shape:\n","# here, 20-dimensional vectors.\n","model.add(Dense(32, activation='relu', input_dim=20))\n","model.add(Dense(10, activation='softmax'))\n","\n","sgd = SGD(lr=0.0001)  # define a learning rate for optimization\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=sgd,\n","              metrics=['accuracy'])\n","\n","model.fit(x_train, y_train,\n","          epochs=20,\n","          batch_size=128)\n","score = model.evaluate(x_test, y_test, batch_size=128) # extract loss and accuracy from test data evaluation\n","print(score)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/20\n","8/8 [==============================] - 0s 2ms/step - loss: 2.3333 - accuracy: 0.1090\n","Epoch 2/20\n","8/8 [==============================] - 0s 2ms/step - loss: 2.3333 - accuracy: 0.1090\n","Epoch 3/20\n","8/8 [==============================] - 0s 2ms/step - loss: 2.3332 - accuracy: 0.1090\n","Epoch 4/20\n","8/8 [==============================] - 0s 2ms/step - loss: 2.3332 - accuracy: 0.1090\n","Epoch 5/20\n","8/8 [==============================] - 0s 2ms/step - loss: 2.3332 - accuracy: 0.1090\n","Epoch 6/20\n","8/8 [==============================] - 0s 3ms/step - loss: 2.3331 - accuracy: 0.1090\n","Epoch 7/20\n","8/8 [==============================] - 0s 2ms/step - loss: 2.3331 - accuracy: 0.1090\n","Epoch 8/20\n","8/8 [==============================] - 0s 2ms/step - loss: 2.3331 - accuracy: 0.1090\n","Epoch 9/20\n","8/8 [==============================] - 0s 2ms/step - loss: 2.3331 - accuracy: 0.1090\n","Epoch 10/20\n","8/8 [==============================] - 0s 2ms/step - loss: 2.3330 - accuracy: 0.1090\n","Epoch 11/20\n","8/8 [==============================] - 0s 3ms/step - loss: 2.3330 - accuracy: 0.1080\n","Epoch 12/20\n","8/8 [==============================] - 0s 2ms/step - loss: 2.3330 - accuracy: 0.1080\n","Epoch 13/20\n","8/8 [==============================] - 0s 2ms/step - loss: 2.3330 - accuracy: 0.1080\n","Epoch 14/20\n","8/8 [==============================] - 0s 2ms/step - loss: 2.3329 - accuracy: 0.1080\n","Epoch 15/20\n","8/8 [==============================] - 0s 2ms/step - loss: 2.3329 - accuracy: 0.1080\n","Epoch 16/20\n","8/8 [==============================] - 0s 2ms/step - loss: 2.3329 - accuracy: 0.1080\n","Epoch 17/20\n","8/8 [==============================] - 0s 2ms/step - loss: 2.3328 - accuracy: 0.1080\n","Epoch 18/20\n","8/8 [==============================] - 0s 2ms/step - loss: 2.3328 - accuracy: 0.1080\n","Epoch 19/20\n","8/8 [==============================] - 0s 2ms/step - loss: 2.3328 - accuracy: 0.1080\n","Epoch 20/20\n","8/8 [==============================] - 0s 2ms/step - loss: 2.3328 - accuracy: 0.1080\n","1/1 [==============================] - 0s 104ms/step - loss: 2.3592 - accuracy: 0.1200\n","[2.3591809272766113, 0.11999999731779099]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fxEI52VDAH9w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623264131594,"user_tz":240,"elapsed":264,"user":{"displayName":"Michael D. Parrott","photoUrl":"","userId":"13070067452828357902"}},"outputId":"09a773be-682e-40f5-a82f-a1c4c0241b6a"},"source":["import numpy as np\n","\n","# Prediction from keras classification model\n","print(x_test.shape)\n","\n","\n","# for predicted probabilities\n","ypreds = model.predict(x_test)\n","print(ypreds) #gives prediction of each category, largest is selected for predict_classes()\n","\n","\n","# for predicted label index of one hot encoded y data columns\n","# Can use this to return correct label from well ordered list of labels\n","ypreds_classindex = np.argmax(model.predict(x_test), axis=-1)\n","print(ypreds_classindex) \n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(100, 20)\n","[[0.10910595 0.0992856  0.09664922 0.09085824 0.10536308 0.08032496\n","  0.07902277 0.14475955 0.08989338 0.10473721]\n"," [0.0811478  0.12187625 0.0892299  0.11844353 0.08290863 0.09310276\n","  0.08209739 0.1277225  0.09848572 0.10498548]\n"," [0.07576741 0.08731587 0.1347216  0.10241567 0.09398641 0.12010638\n","  0.09004815 0.08705765 0.10938957 0.09919122]\n"," [0.09137447 0.13327979 0.08883181 0.09253807 0.13302878 0.08125495\n","  0.07564563 0.09228158 0.11241322 0.09935169]\n"," [0.09011329 0.10666039 0.07792052 0.12349065 0.08788353 0.11314587\n","  0.09368435 0.14432266 0.07855757 0.08422121]\n"," [0.09247523 0.13584821 0.07808033 0.09909981 0.09973314 0.08372197\n","  0.07773024 0.13645275 0.10463145 0.09222684]\n"," [0.07832035 0.13879171 0.08004744 0.09292311 0.11182059 0.10921212\n","  0.10289326 0.0936227  0.1250287  0.06733994]\n"," [0.06697539 0.12378047 0.12131424 0.09666865 0.09477978 0.08948392\n","  0.09933857 0.09051541 0.12978017 0.08736333]\n"," [0.07128386 0.06718138 0.1152059  0.12963459 0.07785166 0.09633655\n","  0.09902824 0.11217687 0.10705256 0.12424843]\n"," [0.08696923 0.12264604 0.09701254 0.10548197 0.11252956 0.064259\n","  0.0736158  0.09467018 0.12801313 0.11480247]\n"," [0.07292171 0.08832575 0.11750735 0.12108089 0.08950841 0.09773305\n","  0.10112052 0.0962329  0.09472137 0.12084804]\n"," [0.08126906 0.1010361  0.0994115  0.11420141 0.09141274 0.08082779\n","  0.08792851 0.09518038 0.14852984 0.10020269]\n"," [0.09936142 0.15393649 0.08826059 0.08630768 0.0981556  0.09106911\n","  0.06776116 0.11196714 0.14186507 0.06131568]\n"," [0.0802998  0.10049616 0.09226838 0.12197327 0.09426913 0.14191304\n","  0.09094306 0.11701419 0.08261395 0.07820906]\n"," [0.11365501 0.10447693 0.0790199  0.10601934 0.11902082 0.08788132\n","  0.07830337 0.11993625 0.10796929 0.08371776]\n"," [0.08495305 0.08817704 0.11379948 0.11509331 0.099751   0.09538648\n","  0.09633329 0.10441143 0.09574246 0.10635243]\n"," [0.10005808 0.12374515 0.06997839 0.13253509 0.08367214 0.08143461\n","  0.0931341  0.1121359  0.11038811 0.0929185 ]\n"," [0.10388566 0.10021862 0.07295151 0.1261132  0.0860296  0.09385765\n","  0.06751111 0.16180655 0.07982071 0.1078054 ]\n"," [0.10897435 0.1040571  0.0972914  0.10565066 0.09395017 0.06281921\n","  0.07643616 0.14829502 0.09844427 0.10408169]\n"," [0.08309632 0.09711933 0.10269875 0.1060793  0.08876996 0.12245219\n","  0.09339558 0.11500724 0.10044419 0.09093723]\n"," [0.08470047 0.1500132  0.08679138 0.09301959 0.10096685 0.0603647\n","  0.06082947 0.09928145 0.12204316 0.14198977]\n"," [0.10487776 0.07975521 0.09470356 0.11759549 0.09072873 0.09323205\n","  0.08722828 0.10035077 0.1361631  0.09536506]\n"," [0.06175585 0.12700158 0.10203064 0.08915646 0.0794356  0.12024982\n","  0.08350471 0.0870624  0.11433817 0.13546476]\n"," [0.08385764 0.08233402 0.10493058 0.1294541  0.07048122 0.12490934\n","  0.1047252  0.13102846 0.09009799 0.07818134]\n"," [0.07801941 0.0794408  0.10722253 0.11886882 0.08809397 0.09844271\n","  0.09903949 0.08992718 0.12626928 0.1146758 ]\n"," [0.07671446 0.10203414 0.11454941 0.10714255 0.09550311 0.12002145\n","  0.11501397 0.13020794 0.07683562 0.06197729]\n"," [0.05682641 0.09808744 0.12253619 0.11680485 0.09904399 0.11885358\n","  0.10785471 0.074638   0.10719255 0.09816234]\n"," [0.0804314  0.10759077 0.11402339 0.10667241 0.11218248 0.05439141\n","  0.07993255 0.10066747 0.10503787 0.13907024]\n"," [0.07649216 0.13651279 0.07046502 0.10590708 0.08619791 0.08805056\n","  0.09972902 0.11755857 0.13458557 0.08450136]\n"," [0.07308789 0.13605843 0.09685604 0.10957546 0.09577354 0.10561914\n","  0.06978382 0.08945012 0.0928352  0.13096036]\n"," [0.08245322 0.14111377 0.08872361 0.09523384 0.09514604 0.06971926\n","  0.09184421 0.10247721 0.12233976 0.11094904]\n"," [0.07089479 0.10650535 0.11258226 0.10178069 0.10327572 0.1012724\n","  0.07523115 0.08595175 0.10377883 0.13872708]\n"," [0.05910993 0.15893112 0.08462041 0.10215951 0.08425492 0.07604963\n","  0.08373171 0.08363315 0.15404102 0.11346855]\n"," [0.08343213 0.10713615 0.1002707  0.13123158 0.10441394 0.09825388\n","  0.08975652 0.10798205 0.09173459 0.08578844]\n"," [0.09012173 0.08024346 0.09577902 0.11711408 0.08258314 0.07022896\n","  0.07851264 0.15260522 0.10562435 0.12718731]\n"," [0.08965074 0.13819756 0.08959059 0.10181041 0.09660748 0.08651519\n","  0.08460081 0.13671274 0.09125385 0.08506062]\n"," [0.05894286 0.12621759 0.09780113 0.08590842 0.09931041 0.08731612\n","  0.0970602  0.08896903 0.14216083 0.11631338]\n"," [0.09550484 0.09791411 0.09371076 0.1148247  0.09291153 0.08820789\n","  0.08443879 0.10109751 0.10862104 0.12276886]\n"," [0.08659876 0.14502066 0.09115565 0.09840253 0.11280143 0.07933436\n","  0.078688   0.09294479 0.10720603 0.10784778]\n"," [0.08006439 0.08711899 0.10968516 0.10732466 0.08413804 0.11070955\n","  0.08906116 0.11462554 0.09469492 0.12257756]\n"," [0.09639523 0.10428606 0.09361921 0.11588326 0.10990991 0.10698347\n","  0.09810263 0.12907541 0.08086641 0.06487846]\n"," [0.06527659 0.13023277 0.08962772 0.11406345 0.08280609 0.1323586\n","  0.10548966 0.10524542 0.07839324 0.09650643]\n"," [0.08187415 0.08434723 0.10036372 0.13136743 0.08789079 0.1387419\n","  0.10661483 0.12164216 0.07876759 0.06839016]\n"," [0.08640615 0.08291078 0.10201404 0.10907459 0.10434401 0.12128367\n","  0.09159741 0.10567512 0.10520659 0.09148758]\n"," [0.09327072 0.0886993  0.09591173 0.1246581  0.08064444 0.10974643\n","  0.07798413 0.09757035 0.09824675 0.13326809]\n"," [0.07139293 0.12461113 0.08403357 0.1154056  0.08908288 0.13186555\n","  0.07835489 0.1217586  0.07900983 0.10448498]\n"," [0.10445537 0.10298283 0.08839781 0.09987105 0.09915175 0.08365475\n","  0.06906351 0.12617907 0.10329667 0.12294716]\n"," [0.08978861 0.12352479 0.09530001 0.1039998  0.12649    0.05498156\n","  0.06329258 0.09626441 0.09496224 0.151396  ]\n"," [0.07994138 0.13350473 0.07445947 0.10665633 0.07737768 0.09056959\n","  0.08070096 0.12271822 0.11295732 0.12111426]\n"," [0.08438495 0.13822988 0.09488272 0.0970769  0.08594144 0.09792546\n","  0.06773026 0.10333596 0.11634171 0.11415068]\n"," [0.10026052 0.10876969 0.09502198 0.12327141 0.09712996 0.10247763\n","  0.06916174 0.09770458 0.10661597 0.09958645]\n"," [0.07253991 0.10134552 0.09145778 0.11180044 0.07140345 0.09394129\n","  0.07604018 0.11319736 0.10327284 0.16500123]\n"," [0.07265672 0.12737821 0.0777199  0.10982101 0.07823722 0.11193705\n","  0.07863349 0.10836696 0.10379542 0.13145396]\n"," [0.07294107 0.12785292 0.07934426 0.11099946 0.10093042 0.11573038\n","  0.1047632  0.09764415 0.11399892 0.0757952 ]\n"," [0.09098364 0.10064092 0.09059144 0.11201984 0.10598351 0.10815478\n","  0.10688412 0.10358419 0.10414969 0.07700794]\n"," [0.10582226 0.10788853 0.08588438 0.11274742 0.10206603 0.09291606\n","  0.08923506 0.13277009 0.09724431 0.0734259 ]\n"," [0.09766984 0.10364704 0.07968678 0.13836744 0.08075742 0.09213152\n","  0.076154   0.13405147 0.10047881 0.0970557 ]\n"," [0.06413111 0.07761437 0.11723682 0.11799409 0.07477178 0.15590374\n","  0.09141272 0.10266435 0.07707342 0.12119754]\n"," [0.06684773 0.12131225 0.11067139 0.09799042 0.09772282 0.06857596\n","  0.09168786 0.11530112 0.12261653 0.10727395]\n"," [0.08710282 0.06916552 0.10450438 0.12917422 0.07808412 0.07890544\n","  0.10913553 0.12785296 0.11688773 0.09918731]\n"," [0.08474465 0.12428094 0.08472111 0.11739738 0.09412372 0.07878049\n","  0.07187783 0.10282705 0.11623677 0.12501006]\n"," [0.10308507 0.0948235  0.10166051 0.09980407 0.09803802 0.08747858\n","  0.08820783 0.13163884 0.09372153 0.10154213]\n"," [0.09169494 0.11539274 0.10475729 0.11480262 0.11862205 0.0717136\n","  0.08853618 0.08780755 0.11521935 0.09145366]\n"," [0.06525393 0.09654304 0.1031762  0.12066707 0.07981138 0.11030298\n","  0.08061451 0.09903036 0.10787181 0.13672872]\n"," [0.09747898 0.11920085 0.07695639 0.09965795 0.10235897 0.0740141\n","  0.05857166 0.11704878 0.12132814 0.13338418]\n"," [0.09639557 0.10850363 0.09717338 0.10834443 0.0949449  0.09236752\n","  0.09246802 0.11140232 0.10898611 0.08941413]\n"," [0.08241119 0.13929564 0.0959451  0.0837884  0.11340938 0.07152911\n","  0.06699296 0.09143829 0.14033955 0.11485035]\n"," [0.09949318 0.09406178 0.09321347 0.11784443 0.09204511 0.07677231\n","  0.08503646 0.12398171 0.11043023 0.10712143]\n"," [0.09791782 0.1029852  0.08635852 0.12099943 0.11198657 0.07504655\n","  0.09469108 0.09289977 0.11891059 0.09820453]\n"," [0.0689954  0.15173055 0.0854806  0.11130572 0.07497888 0.09818197\n","  0.07153215 0.09603766 0.11446406 0.12729296]\n"," [0.05969611 0.08367996 0.12371428 0.13028592 0.0712444  0.11357119\n","  0.08744971 0.10986501 0.11064248 0.10985089]\n"," [0.09670104 0.12181447 0.09630644 0.0962292  0.09967507 0.05100485\n","  0.06622067 0.152079   0.11160418 0.10836509]\n"," [0.07154817 0.09681936 0.14907788 0.11437524 0.09486417 0.12071123\n","  0.09832189 0.08010527 0.08622839 0.08794843]\n"," [0.08672188 0.0709557  0.10302775 0.11705334 0.08152645 0.11088128\n","  0.09573811 0.1145499  0.1117804  0.10776519]\n"," [0.0720868  0.08073493 0.1246758  0.11043236 0.10246716 0.12236861\n","  0.12134609 0.09071206 0.09068237 0.08449385]\n"," [0.1078753  0.10255989 0.09715836 0.1090108  0.08914994 0.07578304\n","  0.09171027 0.12206369 0.12268703 0.08200163]\n"," [0.06558087 0.08693685 0.12129451 0.12885423 0.08384215 0.1161503\n","  0.08538552 0.11590441 0.07986572 0.11618539]\n"," [0.08518926 0.08494645 0.092514   0.11287756 0.09290864 0.09972613\n","  0.08033755 0.12282899 0.09908662 0.12958485]\n"," [0.07302612 0.12396485 0.09602772 0.10724047 0.08785015 0.06914683\n","  0.10070423 0.12096041 0.13008474 0.09099449]\n"," [0.08858482 0.1246588  0.08594041 0.08515817 0.11598543 0.09729934\n","  0.08740332 0.08511806 0.13864124 0.09121044]\n"," [0.08034372 0.11386941 0.09853952 0.10551637 0.10185418 0.09868915\n","  0.08722956 0.1009879  0.10476884 0.10820137]\n"," [0.09062028 0.07572445 0.11005975 0.12092999 0.08054454 0.11449522\n","  0.10714483 0.11938201 0.09616751 0.08493143]\n"," [0.08951273 0.10114902 0.1017555  0.10856352 0.09424783 0.09597681\n","  0.07896537 0.10210311 0.112679   0.11504716]\n"," [0.10438794 0.09772491 0.09859954 0.10870484 0.09391753 0.07767325\n","  0.08735817 0.10596927 0.13035023 0.09531426]\n"," [0.07310143 0.09397352 0.10546877 0.13207503 0.07604015 0.0868864\n","  0.08939843 0.14000285 0.09060912 0.11244425]\n"," [0.07087076 0.08115783 0.13999021 0.10008165 0.09265648 0.10121033\n","  0.09583399 0.08433143 0.13657767 0.09728953]\n"," [0.08451641 0.08165038 0.09295978 0.11660595 0.08982056 0.13991609\n","  0.11114671 0.12061097 0.08125783 0.0815153 ]\n"," [0.08430486 0.15869841 0.07875128 0.08778622 0.10412291 0.06774279\n","  0.08229677 0.09286202 0.14118896 0.10224574]\n"," [0.11814462 0.09452789 0.08371242 0.09383342 0.11718065 0.07560918\n","  0.09350297 0.11395241 0.13721788 0.07231853]\n"," [0.07768623 0.10654302 0.0891839  0.12509103 0.08417079 0.12736443\n","  0.07530101 0.11392829 0.08056702 0.1201643 ]\n"," [0.08676899 0.16339809 0.06667854 0.10268056 0.10708468 0.05513944\n","  0.06946485 0.08544289 0.13697425 0.12636779]\n"," [0.12510827 0.09674521 0.08088719 0.11158244 0.10733762 0.08891132\n","  0.0837431  0.12344241 0.08636082 0.09588155]\n"," [0.09778513 0.13445951 0.09451459 0.09587146 0.10599258 0.10200166\n","  0.07337268 0.1079736  0.10092185 0.08710699]\n"," [0.11152501 0.09094317 0.08871023 0.09928404 0.10151231 0.07596855\n","  0.07159989 0.13962528 0.09876033 0.12207115]\n"," [0.07633788 0.10913695 0.07653914 0.14695293 0.07154021 0.13796677\n","  0.08549094 0.12597153 0.06864719 0.10141648]\n"," [0.08408017 0.09516665 0.13167578 0.08984786 0.1253721  0.09220503\n","  0.08000193 0.08882515 0.09826977 0.11455552]\n"," [0.09726541 0.11150414 0.08349876 0.09800731 0.09158291 0.07844771\n","  0.08795134 0.12068092 0.1339474  0.09711412]\n"," [0.10042712 0.08431142 0.09193683 0.10778765 0.10050336 0.11146428\n","  0.10118841 0.11084562 0.11697917 0.07455617]\n"," [0.06485652 0.08261962 0.125705   0.13800791 0.08289336 0.10411634\n","  0.09711218 0.08832221 0.10368092 0.11268584]\n"," [0.06992468 0.09797554 0.10224417 0.13145564 0.07730842 0.09616088\n","  0.08215062 0.08549464 0.1248946  0.13239078]]\n","[7 7 2 1 7 7 1 8 3 8 3 8 1 5 7 3 3 7 7 5 1 8 9 7 8 7 2 9 1 1 1 9 1 3 7 1 8\n"," 9 1 9 7 5 5 5 9 5 7 9 1 1 3 9 9 1 3 7 3 5 8 3 9 7 4 9 9 7 8 7 3 1 3 7 2 3\n"," 2 8 3 9 8 8 1 3 9 8 7 2 5 1 8 5 1 0 1 7 3 2 8 8 3 9]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rXYym6Qfthqk"},"source":["\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"HH52oduKBPqJ","executionInfo":{"status":"ok","timestamp":1623264131597,"user_tz":240,"elapsed":37,"user":{"displayName":"Michael D. Parrott","photoUrl":"","userId":"13070067452828357902"}},"outputId":"b31135f9-8ee4-44ab-ec00-5af0d0f7d1e5"},"source":["labels=[\"label1\",\"label2\"]\n","\n","labels[1]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'label2'"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"ZrHFjTzAAH9z"},"source":["# Prediction from keras regression model\n","\n","# for predicted probabilities and labels\n","ypreds = model.predict(x_test)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"75CVv3qUAH91"},"source":["## Evaluate and predict keras model with sklearn wrapper"]},{"cell_type":"code","metadata":{"id":"AOnoCE4YAH92","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623274990039,"user_tz":240,"elapsed":168,"user":{"displayName":"Michael D. Parrott","photoUrl":"","userId":"13070067452828357902"}},"outputId":"730e576b-9368-4cf4-a2eb-acaf4ec71bf5"},"source":["# Use scikit-learn to grid search the batch size and epochs\n","from sklearn.model_selection import GridSearchCV\n","from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n","import numpy\n","import pandas as pd\n","# Use KerasRegressor for regression model tuning\n","\n","# fix random seed for reproducibility\n","seed = 7\n","numpy.random.seed(seed)\n","\n","# load pimas diabetes dataset\n","dataset = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/MASS/Pima.te.csv\", delimiter=\",\")\n","\n","X = dataset.iloc[:,1:6]\n","Y = dataset.iloc[:,7]\n","print(X.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(332, 5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6Ld6vwpxAH94","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623275229514,"user_tz":240,"elapsed":24707,"user":{"displayName":"Michael D. Parrott","photoUrl":"","userId":"13070067452828357902"}},"outputId":"216d14d1-0fa7-425e-d6f9-6d4a8fa8dafb"},"source":["# simple example\n","\n","# Function to create model, required for KerasClassifier\n","def create_model(hiddennodes=1):\n","\t# create model\n","\tmodel = Sequential()\n","\tmodel.add(Dense(hiddennodes, input_dim=5, activation='relu'))\n","\tmodel.add(Dense(8, activation='relu'))\n","\tmodel.add(Dense(1, activation='sigmoid'))\n","\t# Compile model\n","\tmodel.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n","\treturn model\n","\n","model = KerasClassifier(build_fn=create_model, epochs=100, verbose=0) # epochs arg is built in to Scikit learn's... \n","                                                                      # KerasClassifier\n","\n","# Building a simple search grid that adjusts epochs\n","param_grid = dict(hiddennodes=[10,20,30])\n","grid = GridSearchCV(estimator=model, param_grid=param_grid)\n","grid_result = grid.fit(X, Y)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:5 out of the last 708 calls to <function Model.make_test_function.<locals>.test_function at 0x7f51cd24ca70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f51cd24c7a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R3nVEw3QAH96","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623264854929,"user_tz":240,"elapsed":195,"user":{"displayName":"Michael D. Parrott","photoUrl":"","userId":"13070067452828357902"}},"outputId":"2797a982-5356-4d2f-e302-754e57b3d3c7"},"source":["# grid_result.cv_results_ for full results file\n","print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Best: 0.081411 using {'hiddennodes': 10}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"54JFe9xnAH9-"},"source":["## Tuning different parameters"]},{"cell_type":"code","metadata":{"id":"9uK16b2eAH9_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623265033286,"user_tz":240,"elapsed":9373,"user":{"displayName":"Michael D. Parrott","photoUrl":"","userId":"13070067452828357902"}},"outputId":"506a0bd2-a75f-45a4-9c7d-ec304957cfb8"},"source":["\n","# Create function that builds model\n","# Function to create model, required for KerasClassifier\n","\n","#In order to tune parameters native to keras, add them as arguments to your create_model function\n","\n","def create_model(learn_rate=0.01):\n","\t# create model\n","\tmodel = Sequential()\n","\tmodel.add(Dense(12, input_dim=5, activation='relu'))\n","\tmodel.add(Dense(1, activation='sigmoid'))\n","\t# Compile model\n","\toptimizer = SGD(learning_rate=learn_rate)\n","\tmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","\treturn model\n","\n","#call model function in KerasClassifier\n","model = KerasClassifier(build_fn=create_model, epochs=20, verbose=0)\n","\n","# define the grid search parameters\n","learn_rate = [0.001, 0.01]\n","\n","param_grid = dict(learn_rate=learn_rate) # set dictionary using function this time\n","\n","#Using n_jobs=-1 to parallelize across available processors to speed it up\n","grid = GridSearchCV(estimator=model, param_grid=param_grid)\n","grid_result = grid.fit(X, Y)\n","\n","# summarize results\n","print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Best: 0.081411 using {'learn_rate': 0.001}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZSq8BmngAH-C"},"source":["## Now you try.  Can you fit a neural network model to the Iris dataset?  Run models that change the structure of the network (i.e.-hidden layers and activations).  Try to improve your validation accuracy as much as possible.\n","\n","Data can be imported via the following link:\n","\n","http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv"]},{"cell_type":"code","metadata":{"id":"tPGjX9vcAH-D","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1623275744611,"user_tz":240,"elapsed":334,"user":{"displayName":"Michael D. Parrott","photoUrl":"","userId":"13070067452828357902"}},"outputId":"2a2026f1-26dc-474a-ef8d-3883deca9ba9"},"source":["\n","import pandas as pd\n","\n","data = pd.read_csv(\"http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv\")\n","\n","\n","#update data to set up for train test split\n","data = data.iloc[:,1:]\n","y = data['Species']\n","X = data.loc[:, data.columns != 'Species']\n","\n","display(pd.get_dummies(y))\n","display(data.head())\n","display(X.head())\n","display(y[0::10])\n","\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>setosa</th>\n","      <th>versicolor</th>\n","      <th>virginica</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>145</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>146</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>147</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>148</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>149</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>150 rows  3 columns</p>\n","</div>"],"text/plain":["     setosa  versicolor  virginica\n","0         1           0          0\n","1         1           0          0\n","2         1           0          0\n","3         1           0          0\n","4         1           0          0\n","..      ...         ...        ...\n","145       0           0          1\n","146       0           0          1\n","147       0           0          1\n","148       0           0          1\n","149       0           0          1\n","\n","[150 rows x 3 columns]"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sepal.Length</th>\n","      <th>Sepal.Width</th>\n","      <th>Petal.Length</th>\n","      <th>Petal.Width</th>\n","      <th>Species</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5.1</td>\n","      <td>3.5</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.9</td>\n","      <td>3.0</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4.7</td>\n","      <td>3.2</td>\n","      <td>1.3</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4.6</td>\n","      <td>3.1</td>\n","      <td>1.5</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5.0</td>\n","      <td>3.6</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species\n","0           5.1          3.5           1.4          0.2  setosa\n","1           4.9          3.0           1.4          0.2  setosa\n","2           4.7          3.2           1.3          0.2  setosa\n","3           4.6          3.1           1.5          0.2  setosa\n","4           5.0          3.6           1.4          0.2  setosa"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sepal.Length</th>\n","      <th>Sepal.Width</th>\n","      <th>Petal.Length</th>\n","      <th>Petal.Width</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5.1</td>\n","      <td>3.5</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.9</td>\n","      <td>3.0</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4.7</td>\n","      <td>3.2</td>\n","      <td>1.3</td>\n","      <td>0.2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4.6</td>\n","      <td>3.1</td>\n","      <td>1.5</td>\n","      <td>0.2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5.0</td>\n","      <td>3.6</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n","0           5.1          3.5           1.4          0.2\n","1           4.9          3.0           1.4          0.2\n","2           4.7          3.2           1.3          0.2\n","3           4.6          3.1           1.5          0.2\n","4           5.0          3.6           1.4          0.2"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["0          setosa\n","10         setosa\n","20         setosa\n","30         setosa\n","40         setosa\n","50     versicolor\n","60     versicolor\n","70     versicolor\n","80     versicolor\n","90     versicolor\n","100     virginica\n","110     virginica\n","120     virginica\n","130     virginica\n","140     virginica\n","Name: Species, dtype: object"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"Ak12E635AH-I"},"source":["\n"],"execution_count":null,"outputs":[]}]}