{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Feature Selection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RLgIT-Dsli1"
      },
      "source": [
        "# Feature Selection Using Models Learned Thus Far..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAqw-dhRsli7"
      },
      "source": [
        "First, Feature selection using SelectFromModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9Ra1zefsli_"
      },
      "source": [
        "SelectFromModel is a meta-transformer that can be used along with any estimator that has a coef_ or feature_importances_ \n",
        "attribute after fitting. \n",
        "\n",
        "The features are considered unimportant and removed, if the corresponding coef_ or feature_importances_ values are below \n",
        "the provided threshold parameter. \n",
        "\n",
        "Apart from specifying the threshold numerically, there are built-in heuristics for finding a threshold using a string argument. \n",
        "\n",
        "Available heuristics are “mean”, “median” and float multiples of these like “0.1*mean”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyJIRX7JsljG"
      },
      "source": [
        "### Example 1: Fit a Random Forest model and use SelectFromModel to keep important features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGXHOxYHsljI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e07ea8bc-acfd-4407-975f-29cfb25271a9"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "boston = load_boston()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(boston.data, boston.target,\n",
        "                                                random_state=0)\n",
        "\n",
        "forest = RandomForestRegressor(n_estimators=200)\n",
        "formodel = forest.fit(Xtrain, ytrain)\n",
        "\n",
        "\n",
        "print(formodel.feature_importances_)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.04028573 0.00141938 0.00817512 0.00168703 0.0161306  0.40572326\n",
            " 0.01278706 0.0374204  0.00317629 0.018024   0.02192886 0.00925718\n",
            " 0.42398507]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCHj5MzHsljc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "329cb518-9213-4e00-fdc8-59fb9fa68ac2"
      },
      "source": [
        "# Set a minimum threshold of 0.25\n",
        "sfm = SelectFromModel(formodel, threshold=.25)\n",
        "sfm.fit(Xtrain, ytrain)\n",
        "Xtrain_new = sfm.transform(Xtrain) # transform data to insert into new model\n",
        "\n",
        "print(Xtrain_new[0:5,:]) #only two variables in X now\n",
        "\n",
        "print(Xtrain.shape) #compare to original data with 13 variables"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 5.605 18.46 ]\n",
            " [ 5.927  9.22 ]\n",
            " [ 7.267  6.05 ]\n",
            " [ 6.471 17.12 ]\n",
            " [ 6.782 25.79 ]]\n",
            "(379, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuUgVm8psljs"
      },
      "source": [
        "### Example 2: Fit a Lasso model and use SelectFromModel to keep important features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "catz5SZCslju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49848688-48b8-4445-bd59-0b004a7d3c9a"
      },
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "lassomodel = Lasso(alpha=10).fit(Xtrain, ytrain)\n",
        "model = SelectFromModel(lassomodel, prefit=True) # prefit argument allows non zero features to be chosen\n",
        "                                                 # from regularized models like lasso\n",
        "    \n",
        "X_new = model.transform(Xtrain) # transform data to insert into new model\n",
        "\n",
        "print(lassomodel.coef_)\n",
        "print(X_new.shape) #down to four variables from 13\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.          0.03268741 -0.          0.          0.          0.\n",
            "  0.         -0.          0.         -0.01155885 -0.          0.00679306\n",
            " -0.54971245]\n",
            "(379, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9evdaLislj_"
      },
      "source": [
        "# Using Recursive Feature Elimination to Choose Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlGqE-L1slkA"
      },
      "source": [
        "Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. \n",
        "\n",
        "First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. \n",
        "\n",
        "Basic algorithm:\n",
        "Start with full model.  Run series of models that evaluate prediction error on ytrain after dropping a feature.  Repeat for all features.  Drop feature that is helps least in predicting ytrain.  Repeat process with n-1 features...\n",
        "\n",
        "Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9klFlhPslkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fb009e3-d805-40f2-b801-0684cf67fcef"
      },
      "source": [
        "#EXAMPLE:  RFE to find 5 features that help model predict the best:\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "estimator = LinearRegression().fit(Xtrain, ytrain) #model with all X variables\n",
        "\n",
        "\n",
        "selector = RFE(estimator, 3, step=1) # step tells RFE how many features to remove each time model features are evaluated\n",
        "\n",
        "selector = selector.fit(Xtrain, ytrain) # fit RFE estimator.\n",
        "\n",
        "print(\"Num Features: \"+str(selector.n_features_))\n",
        "print(\"Selected Features: \"+str(selector.support_)) # T/F for top five features\n",
        "print(\"Feature Ranking: \"+str(selector.ranking_))  # ranking for top five + features"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Features: 3\n",
            "Selected Features: [False False False  True  True  True False False False False False False\n",
            " False]\n",
            "Feature Ranking: [ 5  7 11  1  1  1 10  3  6  8  2  9  4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGDUZRxLslkW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a847721-9047-4b0f-9c87-6a6ec76c8282"
      },
      "source": [
        "# Transform X data for other use in this model or other models:\n",
        "\n",
        "Xnew = selector.transform( Xtrain) #reduces X to subset identified above\n",
        "boston.feature_names[selector.support_ ] # five most important features"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['CHAS', 'NOX', 'RM'], dtype='<U7')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jtUlKeeslkl"
      },
      "source": [
        "## Can you use feature selection to transform the following dataset using different feature selection techniques?  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SpckUUrslks"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "bc = load_breast_cancer()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Axqy3j5Lslk7"
      },
      "source": [
        "X=bc.data\n",
        "y = bc.target"
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}