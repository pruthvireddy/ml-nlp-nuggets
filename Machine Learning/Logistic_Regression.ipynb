{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"Logistic_Regression.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"WmugLEKqw82G"},"source":["# Logistic Regression"]},{"cell_type":"code","metadata":{"id":"p68n9lV6w84H"},"source":["#Import some example data\n","\n","import pandas as pd\n","# target = InMichelin, whether or not a restaurant is in the Michelin guide\n","data = pd.read_csv(\"http://gattonweb.uky.edu/sheather/book/docs/datasets/MichelinNY.csv\" , encoding=\"latin_1\")\n","data.head()\n","\n","#update data to set up for train test split\n","data = data.loc[:, data.columns != 'Restaurant Name']\n","y = data['InMichelin']\n","X= data.loc[:, data.columns != 'InMichelin']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CONzsi2Ww89_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"eb54ef85-2a71-4827-ceaa-8d80534db2e7"},"source":["#Set up training and test data\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) \n","\n","#Note: random_state ensures same data will be generated for example each time\n","\n","#Note: logistic regression in sklearn is preset to be a regularization model with C=100).\n","#If you make C really high the model effectively becomes a logistic regression model...\n","\n","logreg = LogisticRegression(C=1e90).fit(X_train, y_train)\n","\n","print(\"logreg .coef_: {}\".format(logreg .coef_))\n","\n","\n","print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n","print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n","\n","\n","predicted_vals = logreg.predict(X_test) # y_pred includes your predictions\n","print(\"logreg.predict: {}\".format(predicted_vals))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["logreg .coef_: [[ 0.38181614  0.07433425 -0.15691054  0.08189853]]\n","Training set score: 0.797\n","Test set score: 0.780\n","logreg.predict: [0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0\n"," 1 1 1 1]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uf04mc5Ww9Bo","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4f6944a5-6feb-4402-f80a-ccfb28eab35b"},"source":["logreg\n","\n","#Use ?LogisticRegression() for more information"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=1e+90, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n","                   multi_class='auto', n_jobs=None, penalty='l2',\n","                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n","                   warm_start=False)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"q05_hoWuw9CM"},"source":["## Logistic Regression in statsmodels package"]},{"cell_type":"code","metadata":{"id":"4UmAoeiyw9Ce","colab":{"base_uri":"https://localhost:8080/","height":416},"outputId":"61e9e1b6-cdcb-4102-b3e1-2e5721237432"},"source":["import statsmodels.api as sm\n","\n","X_train_new = sm.add_constant(X_train)\n","\n","model = sm.GLM(y_train, X_train_new, family=sm.families.Binomial()).fit()\n","\n","model.summary()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<table class=\"simpletable\">\n","<caption>Generalized Linear Model Regression Results</caption>\n","<tr>\n","  <th>Dep. Variable:</th>      <td>InMichelin</td>    <th>  No. Observations:  </th>  <td>   123</td> \n","</tr>\n","<tr>\n","  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>   118</td> \n","</tr>\n","<tr>\n","  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>     4</td> \n","</tr>\n","<tr>\n","  <th>Link Function:</th>         <td>logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n","</tr>\n","<tr>\n","  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -57.266</td>\n","</tr>\n","<tr>\n","  <th>Date:</th>            <td>Thu, 11 Feb 2021</td> <th>  Deviance:          </th> <td>  114.53</td>\n","</tr>\n","<tr>\n","  <th>Time:</th>                <td>14:26:08</td>     <th>  Pearson chi2:      </th>  <td>  254.</td> \n","</tr>\n","<tr>\n","  <th>No. Iterations:</th>          <td>6</td>        <th>                     </th>     <td> </td>   \n","</tr>\n","<tr>\n","  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n","</tr>\n","</table>\n","<table class=\"simpletable\">\n","<tr>\n","     <td></td>        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n","</tr>\n","<tr>\n","  <th>const</th>   <td>  -10.6490</td> <td>    2.588</td> <td>   -4.115</td> <td> 0.000</td> <td>  -15.722</td> <td>   -5.576</td>\n","</tr>\n","<tr>\n","  <th>Food</th>    <td>    0.3818</td> <td>    0.148</td> <td>    2.572</td> <td> 0.010</td> <td>    0.091</td> <td>    0.673</td>\n","</tr>\n","<tr>\n","  <th>Decor</th>   <td>    0.0743</td> <td>    0.103</td> <td>    0.720</td> <td> 0.471</td> <td>   -0.128</td> <td>    0.277</td>\n","</tr>\n","<tr>\n","  <th>Service</th> <td>   -0.1569</td> <td>    0.147</td> <td>   -1.070</td> <td> 0.285</td> <td>   -0.444</td> <td>    0.131</td>\n","</tr>\n","<tr>\n","  <th>Price</th>   <td>    0.0819</td> <td>    0.036</td> <td>    2.269</td> <td> 0.023</td> <td>    0.011</td> <td>    0.153</td>\n","</tr>\n","</table>"],"text/plain":["<class 'statsmodels.iolib.summary.Summary'>\n","\"\"\"\n","                 Generalized Linear Model Regression Results                  \n","==============================================================================\n","Dep. Variable:             InMichelin   No. Observations:                  123\n","Model:                            GLM   Df Residuals:                      118\n","Model Family:                Binomial   Df Model:                            4\n","Link Function:                  logit   Scale:                          1.0000\n","Method:                          IRLS   Log-Likelihood:                -57.266\n","Date:                Thu, 11 Feb 2021   Deviance:                       114.53\n","Time:                        14:26:08   Pearson chi2:                     254.\n","No. Iterations:                     6                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          z      P>|z|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const        -10.6490      2.588     -4.115      0.000     -15.722      -5.576\n","Food           0.3818      0.148      2.572      0.010       0.091       0.673\n","Decor          0.0743      0.103      0.720      0.471      -0.128       0.277\n","Service       -0.1569      0.147     -1.070      0.285      -0.444       0.131\n","Price          0.0819      0.036      2.269      0.023       0.011       0.153\n","==============================================================================\n","\"\"\""]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"JpPCv3Trw9DE"},"source":["## Logistic Regression with constraints on size of coefficients"]},{"cell_type":"code","metadata":{"id":"relLZvdNw9DL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"166af6ef-7d5b-4fc0-8712-1303525433a5"},"source":["# Smaller C will constrain Betas more.  It's a tuning parameter we can find using gridsearch.\n","\n","#C=100, compare coefs to regular model above.\n","logreg = LogisticRegression(C=100).fit(X_train, y_train)\n","\n","print(\"logreg .coef_: {}\".format(logreg .coef_))\n","\n","\n","print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n","print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n","\n","\n","predicted_vals = logreg.predict(X_test) # y_pred includes your predictions\n","print(\"logreg.predict: {}\".format(predicted_vals))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["logreg .coef_: [[ 0.38171368  0.07433904 -0.15682846  0.08189077]]\n","Training set score: 0.797\n","Test set score: 0.780\n","logreg.predict: [0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0\n"," 1 1 1 1]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lNqvfuhzw9Dg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"432133a7-7636-403d-b466-e1a9fd8d116e"},"source":["\n","#C=1, compare coefs to above models.\n","logreg = LogisticRegression(C=.001).fit(X_train, y_train)\n","\n","print(\"logreg .coef_: {}\".format(logreg .coef_))\n","\n","\n","print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n","print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n","\n","\n","predicted_vals = logreg.predict(X_test) # y_pred includes your predictions\n","print(\"logreg.predict: {}\".format(predicted_vals))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["logreg .coef_: [[0.02661388 0.02414286 0.01347687 0.06925635]]\n","Training set score: 0.748\n","Test set score: 0.780\n","logreg.predict: [0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0\n"," 1 1 1 1]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RRxPSrzWR4xM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a29420c6-da8c-4fdd-af0c-e041ef84889c"},"source":["logreg"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n","                   multi_class='auto', n_jobs=None, penalty='l2',\n","                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n","                   warm_start=False)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"P7cezPHjw9Dy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"044f0848-e65f-441b-841c-c7257280ea32"},"source":["\n","#C=.01, compare coefs to above models.\n","\n","#Does the model's prediction power get better or worse??\n","\n","logreg = LogisticRegression(C=.01).fit(X_train, y_train)\n","\n","print(\"logreg .coef_: {}\".format(logreg .coef_))\n","\n","\n","print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n","print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n","\n","\n","predicted_vals = logreg.predict(X_test) # y_pred includes your predictions\n","print(\"logreg.predict: {}\".format(predicted_vals))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["logreg .coef_: [[0.12347623 0.06179923 0.00296359 0.07729161]]\n","Training set score: 0.797\n","Test set score: 0.780\n","logreg.predict: [0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0\n"," 1 1 1 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2jEf652Zw9EP"},"source":["### We can also change the default in the penalty argurment from penalty='l2' to penalty='l1' to adjust our regularization constraints:\n","\n","?LogisticRegression()"]},{"cell_type":"markdown","metadata":{"id":"txyY3Bugw9Ea"},"source":["#  Challenge:  How would you use GridsearchCV to tune the C parameter?\n"]},{"cell_type":"code","metadata":{"id":"l1FUzuDiw9Eo","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cd18c76b-e32a-4be5-faaa-3d74bd036463"},"source":["# Here is some example code from our knn tutorial for reference:\n","\n","from sklearn.model_selection import GridSearchCV\n","import numpy as np\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","#create dictionary data object with keys equal to parameter name 'n_neighbors' \n","#for knn model and values equal to range of k values to create models for\n","\n","param_grid = {'n_neighbors': np.arange(1, 15, 2)} #np.arange creates sequence of numbers for each k value\n","\n","grid = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=10)\n","\n","#use meta model methods to fit score and predict model:\n","grid.fit(X_train, y_train)\n","\n","#extract best score and parameter by calling objects \"best_score_\" and \"best_params_\"\n","print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n","print(\"best parameters: {}\".format(grid.best_params_))\n","print(\"test-set score: {:.3f}\".format(grid.score(X_test, y_test)))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["best mean cross-validation score: 0.813\n","best parameters: {'n_neighbors': 13}\n","test-set score: 0.756\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"O5EH951nw9FK"},"source":["## Multiclass models (Multinomial model)"]},{"cell_type":"code","metadata":{"id":"4Q0_pnekw9FV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e2630846-9b88-42d8-f3e1-3c12cc1ca2d8"},"source":["from sklearn.datasets import load_iris\n","import numpy as np\n","\n","iris = load_iris()\n","iris\n","X, y = iris.data, iris.target\n","\n","print(iris.feature_names )# X variable names\n","print(X[0:5]) # first five rows of data\n","\n","print(iris.target_names) #target categories\n","print(np.unique(y)) #target values\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n","[[5.1 3.5 1.4 0.2]\n"," [4.9 3.  1.4 0.2]\n"," [4.7 3.2 1.3 0.2]\n"," [4.6 3.1 1.5 0.2]\n"," [5.  3.6 1.4 0.2]]\n","['setosa' 'versicolor' 'virginica']\n","[0 1 2]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"68GSNxuuw9F0"},"source":["logreg = LogisticRegression(C=1e90,multi_class=\"multinomial\",solver=\"lbfgs\").fit(X,y) #Note the argument changes to LogisticRegression()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D3H0XMATw9GH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0fd09bec-6862-4a1a-a4da-c385a7bcb62d"},"source":["print(logreg.predict(X)) #uses softmax function to predict new X data, but I am being lazy and using X data here."],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2]\n"],"name":"stdout"}]}]}